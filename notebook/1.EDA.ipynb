{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libriary\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# encoder libirary\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "# modeling & evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column as fc\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "def load_data(path):\n",
    "    result = pd.read_csv(path)\n",
    "    return result\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\chend\\\\Documents\\\\case\\\\dealer_txns.csv\"\n",
    "# import data\n",
    "df = load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278337, 25)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    # Create 'PolicyPurchased' column based on 'Returned' column\n",
    "    df['PolicyPurchased'] = np.where(df['Returned'].isna(), 0, 1)\n",
    "    \n",
    "    # Create 'Returned_new' column with filled missing values as 0\n",
    "    df['Returned_new'] = df['Returned'].fillna(0)\n",
    "    \n",
    "    # Convert 'SaleDate' to datetime and extract time information\n",
    "    df['SaleDate'] = pd.to_datetime(df.SaleDate, format='%y-%m-%d')\n",
    "    df['SaleYear'] = df['SaleDate'].dt.year\n",
    "    df['SaleMonth'] = df['SaleDate'].dt.month\n",
    "    df['SaleDay'] = df['SaleDate'].dt.day\n",
    "    df['SaleWeekday'] = df['SaleDate'].dt.weekday\n",
    "    \n",
    "    # Calculate 'Age' based on 'SaleYear' and 'CarYear'\n",
    "    df['Age'] = df['SaleYear'] - df['CarYear'] + 1\n",
    "    \n",
    "    # Filter rows based on 'Autocheck_score' range\n",
    "    df = df[(df[\"Autocheck_score\"] >= -5) & (df[\"Autocheck_score\"] <= 5)].copy()\n",
    "    \n",
    "    # Create 'IsWeekend' column\n",
    "    df['IsWeekend'] = df['SaleWeekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "    # Create 'IsOnline' column\n",
    "    df['IsOnline'] = df.apply(lambda row: 1 if row['OVE'] == 1 or row['Simulcast'] == 1 else 0, axis=1)\n",
    "\n",
    "    # Create 'NumTransactionsBuyer' column\n",
    "    df['NumTransactionsBuyer'] = df.groupby('BuyerID')['BuyerID'].transform('count')\n",
    "\n",
    "    # Create 'CarsBoughtBuyer' column\n",
    "    buyer_counts = df['BuyerID'].value_counts()\n",
    "    df['CarsBoughtBuyer'] = df['BuyerID'].map(buyer_counts)\n",
    "\n",
    "    # Create 'AvgCarAgeByBuyer' column\n",
    "    df['AvgCarAgeByBuyer'] = df.groupby('BuyerID')['Age'].transform('mean')\n",
    "\n",
    "    # Create 'price_diff' column\n",
    "    df['PriceDiff'] = df['SalePrice'] - df['MMR']\n",
    "    \n",
    "    # if MMR is missing, fill with Saleprice\n",
    "    mask = df['MMR'] == 0\n",
    "    df.loc[mask,'MMR'] = df.loc[mask, 'SalePrice']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(266917, 38)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## log tranformation\n",
    "\n",
    "def create_log_columns(df, columns):\n",
    "    for column in columns:\n",
    "        new_column = 'Log' + column\n",
    "        df[new_column] = np.log1p(df[column])\n",
    "    return df\n",
    "\n",
    "log_columns = ['Mileage', 'SalePrice', 'MMR']\n",
    "df = create_log_columns(df, log_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping conditionreport columns\n",
    "def apply_mapping_and_convert_to_numeric(df, column, mapping):\n",
    "    df[column] = df[column].replace(mapping)\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return df\n",
    "\n",
    "mapping = {\"EC\": 60, \"CL\": 50, \"AV\": 40, \"RG\": 30, \"PR\": 20, \"SL\": 10}\n",
    "df = apply_mapping_and_convert_to_numeric(df, \"ConditionReport\", mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### mileage usage per year\n",
    "def calculate_usage_per_year(row):\n",
    "    if row['Age'] == 0:\n",
    "        return row['Mileage']\n",
    "    else:\n",
    "        return row['Mileage'] / row['Age']\n",
    "\n",
    "df['UsagePerYear'] = df.apply(calculate_usage_per_year, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set bin \n",
    "def apply_binning(df, column, bins, labels):\n",
    "    df[column + 'Category'] = pd.cut(df[column], bins=bins, labels=labels)\n",
    "    return df\n",
    "\n",
    "bins = [df.LogMileage.min(), 10, 12, df.LogMileage.max()]\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "df = apply_binning(df, 'LogMileage', bins, labels)\n",
    "\n",
    "bins = [df.PriceDiff.min(), -2500, 0, 2500, df.PriceDiff.max()]\n",
    "labels = ['Much Lower', 'Lower', 'Higher', 'Much Higher']\n",
    "df = apply_binning(df, 'PriceDiff', bins, labels)\n",
    "\n",
    "bins = [df['Autocheck_score'].min(), 0, 1, df['Autocheck_score'].max()]\n",
    "labels = ['Negative', 'Zero', 'Positive']\n",
    "\n",
    "df = apply_binning(df, 'Autocheck_score', bins, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop null values\n",
    "def drop_nan_values(df, columns):\n",
    "    df = df.dropna(subset=columns)\n",
    "    return df\n",
    "\n",
    "columns_to_dropna = ['PriceDiffCategory', 'Autocheck_scoreCategory', 'LogMileageCategory', 'JDPowersCat']\n",
    "df = drop_nan_values(df, columns_to_dropna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## oridinal encoding\n",
    "def ordinal_encode_columns(df, columns, categories):\n",
    "    encoder = OrdinalEncoder(categories=categories)\n",
    "    df[columns] = encoder.fit_transform(df[columns])\n",
    "    return df\n",
    "\n",
    "columns_to_encode = ['PriceDiffCategory', 'Autocheck_scoreCategory', 'LogMileageCategory', 'JDPowersCat']\n",
    "categories = [['Much Lower', 'Lower', 'Higher', 'Much Higher'],\n",
    "              ['Negative', 'Zero', 'Positive'],\n",
    "             ['Low', 'Medium', 'High'],\n",
    "             ['EXCLUDED','FULLSIZE CAR','SPORTS CAR', 'VAN','PICKUP', 'LUXURY CAR', 'COMPACT CAR','MIDSIZE CAR', 'SUV']]\n",
    "\n",
    "df = ordinal_encode_columns(df, columns_to_encode, categories)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create columns 'IsLuxury'\n",
    "def create_is_luxury_column(df, luxury_brands):\n",
    "    df['IsLuxury'] = df['CarMake'].apply(lambda x: 1 if x in luxury_brands else 0)\n",
    "    return df\n",
    "\n",
    "luxury_brands = ['B M W', 'MERCEDES-BENZ', 'LEXUS', 'CADILLAC', 'LINCOLN', 'ACURA', 'AUDI', 'LAND ROVER', 'PORSCHE', 'MASERATI', 'BENTLEY', 'FERRARI', 'ASTON MARTIN']\n",
    "df = create_is_luxury_column(df, luxury_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hashing encoding\n",
    "\n",
    "def hashEncoded(df, columns):\n",
    "    for column in columns:\n",
    "        # Initialize HashingEncoder\n",
    "        encoder = ce.HashingEncoder(n_components=8)\n",
    "\n",
    "        # Fit and transform the column and replace it\n",
    "        encoded = encoder.fit_transform(df[column])\n",
    "        encoded.columns = f'{column}_' + encoded.columns\n",
    "\n",
    "        # Join the encoded dataframe with the original dataframe\n",
    "        df = df.join(encoded)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "columns = ['SellingLocation', 'CarMake']\n",
    "df = hashEncoded(df, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 9.228086042146874\n"
     ]
    }
   ],
   "source": [
    "## conditionreport column is critical\n",
    "## so use exist value to predict null value\n",
    "## Decision tree regressor\n",
    "\n",
    "def predict_missing_condition_report(df):\n",
    "    # Selecting columns that have correlation greater than 0.3 with 'ConditionReport'\n",
    "    corr = df.corr()\n",
    "    selected_columns = corr[abs(corr['ConditionReport']) > 0.3].index\n",
    "\n",
    "    # Dropping 'ConditionReport' from selected_columns\n",
    "    selected_columns = selected_columns.drop('ConditionReport')\n",
    "\n",
    "    # Creating new DataFrame with non-null 'ConditionReport' rows\n",
    "    df_notnull = df[df['ConditionReport'].notnull()]\n",
    "\n",
    "    # Splitting data into train and test\n",
    "    X = df_notnull[selected_columns]\n",
    "    y = df_notnull['ConditionReport']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Initializing and fitting decision tree regressor\n",
    "    tree = DecisionTreeRegressor(random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting 'ConditionReport' for test data and calculating RMSE\n",
    "    y_pred = tree.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "    # Now predicting 'ConditionReport' for rows with null values\n",
    "    df_null = df[df['ConditionReport'].isnull()]\n",
    "    X_null = df_null[selected_columns]\n",
    "    predicted_condition_report = tree.predict(X_null)\n",
    "\n",
    "    # Filling null values in 'ConditionReport' with predicted values\n",
    "    df.loc[df['ConditionReport'].isnull(), 'ConditionReport'] = predicted_condition_report\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = predict_missing_condition_report(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_features(df):\n",
    "    # # price difference percentage\n",
    "    df['PriceDiffPercentage'] = abs(df['PriceDiff'] / df['MMR']) * 100\n",
    "    df['Buyer_Total_Arbitrated'] = df.groupby('BuyerID')['Arbitrated'].transform('sum')\n",
    "    df['Buyer_Average_Arbitrated'] = df.groupby('BuyerID')['Arbitrated'].transform('mean')\n",
    "\n",
    "    df['Buyer_Total_Salvage'] = df.groupby('BuyerID')['Salvage'].transform('sum')\n",
    "    df['Buyer_Average_Salvage'] = df.groupby('BuyerID')['Salvage'].transform('mean')\n",
    "\n",
    "    # Binary features\n",
    "    for feature in ['PSIEligible', 'DSEligible', 'IsLuxury']:\n",
    "        df[f'Buyer_Total_{feature}'] = df.groupby('BuyerID')[feature].transform('sum')\n",
    "        df[f'Buyer_Average_{feature}'] = df.groupby('BuyerID')[feature].transform('mean')\n",
    "\n",
    "    # Continuous features\n",
    "    for feature in ['Autocheck_score', 'UsagePerYear', 'PriceDiffPercentage', 'LogMileageCategory', 'LogSalePrice']:\n",
    "        df[f'Buyer_Total_{feature}'] = df.groupby('BuyerID')[feature].transform('sum')\n",
    "        df[f'Buyer_Average_{feature}'] = df.groupby('BuyerID')[feature].transform('mean')\n",
    "\n",
    "    # Categorical features\n",
    "    for feature in ['SaleMonth', 'JDPowersCat']:\n",
    "        mode = lambda x: x.value_counts().index[0] if len(x.value_counts()) > 0 else np.nan\n",
    "        df[f'Buyer_Mode_{feature}'] = df.groupby('BuyerID')[feature].transform(mode)\n",
    "\n",
    "    # Binary features\n",
    "    for feature in ['Salvage', 'PSIEligible', 'DSEligible', 'IsLuxury']:\n",
    "        df[f'Arbitrated_{feature}'] = df['Arbitrated'] * df[feature]\n",
    "\n",
    "    # Continuous features\n",
    "    for feature in ['UsagePerYear', 'LogMileageCategory', 'LogSalePrice']:\n",
    "        df[f'Arbitrated_{feature}'] = df['Arbitrated'] * df[feature]\n",
    "\n",
    "    # Categorical features\n",
    "    mode = lambda x: x.value_counts().index[0] if len(x.value_counts()) > 0 else np.nan\n",
    "    for feature in ['SaleMonth', 'JDPowersCat']:\n",
    "        df[f'Arbitrated_{feature}'] = df.groupby('Arbitrated')[feature].transform(mode)\n",
    "\n",
    "    df['Buyer_Seller_Price'] = df.groupby(['BuyerID', 'SellerID'])['LogSalePrice'].transform('mean')\n",
    "    df['Buyer_Arbitrated_Transactions'] = df.groupby(['BuyerID', 'Arbitrated'])['NumTransactionsBuyer'].transform('sum')\n",
    "    df['Buyer_Autocheck_Price'] = df.groupby(['BuyerID', 'Autocheck_score'])['LogSalePrice'].transform('mean')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = create_interaction_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_month_interaction_features(df):\n",
    "    mode = lambda x: x.value_counts().index[0] if len(x.value_counts()) > 0 else np.nan\n",
    "    df['Buyer_Make0_Month'] = df.groupby(['BuyerID', 'CarMake_col_0'])['SaleMonth'].transform(mode)\n",
    "    df['Buyer_Make1_Month'] = df.groupby(['BuyerID', 'CarMake_col_1'])['SaleMonth'].transform(mode)\n",
    "    df['Buyer_Make2_Month'] = df.groupby(['BuyerID', 'CarMake_col_2'])['SaleMonth'].transform(mode)\n",
    "    df['Buyer_Make3_Month'] = df.groupby(['BuyerID', 'CarMake_col_3'])['SaleMonth'].transform(mode)\n",
    "    df['Buyer_Make4_Month'] = df.groupby(['BuyerID', 'CarMake_col_4'])['SaleMonth'].transform(mode)\n",
    "    df['Buyer_Make5_Month'] = df.groupby(['BuyerID', 'CarMake_col_5'])['SaleMonth'].transform(mode)\n",
    "    df['Buyer_Make6_Month'] = df.groupby(['BuyerID', 'CarMake_col_6'])['SaleMonth'].transform(mode)\n",
    "    df['Buyer_Make7_Month'] = df.groupby(['BuyerID', 'CarMake_col_7'])['SaleMonth'].transform(mode)\n",
    "    return df\n",
    "\n",
    "df = create_month_interaction_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265980, 105)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop columns not explain the case \n",
    "\n",
    "def drop_columns(df, columns):\n",
    "    df = df.drop(columns, axis=1)\n",
    "    return df\n",
    "\n",
    "columns_to_drop = ['VIN', 'SaleYear', 'Returned', 'SellingLocation', 'CarMake', 'CarYear', 'SaleDate', 'Mileage', 'SalePrice', 'MMR']\n",
    "df = drop_columns(df, columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance (first 30 components): [0.158 0.101 0.075 0.063 0.031 0.03  0.024 0.023 0.02  0.019 0.018 0.017\n",
      " 0.015 0.015 0.014 0.014 0.013 0.013 0.013 0.013 0.012 0.012 0.012 0.012\n",
      " 0.012 0.011 0.011 0.011 0.011 0.011]\n",
      "Cumulative explained variance (first 30 components): [0.158 0.259 0.335 0.398 0.43  0.459 0.483 0.505 0.525 0.544 0.562 0.58\n",
      " 0.595 0.61  0.624 0.638 0.652 0.664 0.677 0.69  0.702 0.714 0.726 0.738\n",
      " 0.749 0.761 0.772 0.783 0.794 0.805]\n"
     ]
    }
   ],
   "source": [
    "def pca_var(df):\n",
    "    # Select the features for PCA\n",
    "    X = df.drop('Returned_new', axis=1)  # Drop the target column\n",
    "    y = df['Returned_new']  # Target column\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    var_exp = pca.explained_variance_ratio_\n",
    "    print(f'Explained variance (first 30 components): {np.round(var_exp[:30],3)}')\n",
    "    \n",
    "    # Generate the cumulative explained variance.\n",
    "    cum_var_exp = np.cumsum(var_exp)\n",
    "    print(f'Cumulative explained variance (first 30 components): {np.round(cum_var_exp[:30],3)}')\n",
    "\n",
    "pca_var(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(df, n_components):\n",
    "    # Select the features for PCA\n",
    "    X = df.drop('Returned_new', axis=1)  # Drop the target column\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # Create a new DataFrame with the reduced dimensions\n",
    "    df_reduced = pd.DataFrame(data=X_pca, columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
    "\n",
    "    return df_reduced\n",
    "\n",
    "\n",
    "df_reduced = apply_pca(df, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PC1     0\n",
       "PC2     0\n",
       "PC3     0\n",
       "PC4     0\n",
       "PC5     0\n",
       "PC6     0\n",
       "PC7     0\n",
       "PC8     0\n",
       "PC9     0\n",
       "PC10    0\n",
       "PC11    0\n",
       "PC12    0\n",
       "PC13    0\n",
       "PC14    0\n",
       "PC15    0\n",
       "PC16    0\n",
       "PC17    0\n",
       "PC18    0\n",
       "PC19    0\n",
       "PC20    0\n",
       "PC21    0\n",
       "PC22    0\n",
       "PC23    0\n",
       "PC24    0\n",
       "PC25    0\n",
       "PC26    0\n",
       "PC27    0\n",
       "PC28    0\n",
       "PC29    0\n",
       "PC30    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6998769987699877\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_reduced, df['Returned_new'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataset us\n",
    "\n",
    "df_new = df[['PolicyPurchased',\n",
    " 'Arbitrated_UsagePerYear',\n",
    " 'Arbitrated_LogSalePrice',\n",
    " 'Arbitrated',\n",
    " 'PSI',\n",
    " 'Arbitrated_DSEligible',\n",
    " 'SaleMonth',\n",
    " 'Arbitrated_LogMileageCategory',\n",
    " 'LIGHTR',\n",
    " 'SellerID',\n",
    " 'Buyer_Average_Arbitrated',\n",
    " 'LogMileage',\n",
    " 'Buyer_Seller_Price',\n",
    " 'PriceDiffPercentage',\n",
    " 'LogSalePrice',\n",
    " 'Arbitrated_PSIEligible',\n",
    " 'LogMMR',\n",
    " 'Autocheck_score',\n",
    " 'UsagePerYear',\n",
    " 'Buyer_Autocheck_Price',\n",
    " 'PriceDiff',\n",
    " 'Buyer_Arbitrated_Transactions',\n",
    " 'SaleDay',\n",
    " 'ConditionReport',\n",
    " 'SaleWeekday',\n",
    " 'Buyer_Total_Arbitrated',\n",
    " 'Age',\n",
    " 'Buyer_Total_LogSalePrice',\n",
    " 'Buyer_Total_Autocheck_score',\n",
    " 'CarsBoughtBuyer', 'Returned_new']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    261871\n",
       "1.0      4109\n",
       "Name: Returned_new, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.Returned_new.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
